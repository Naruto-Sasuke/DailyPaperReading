# Overall
## Deep Learning with S-shaped Rectified Linear Activation Units
This paper proposes a new rectified linear activation unit named SRelu. 
Just a piecewise funciton(three parts). Seems boring.

## Densely Connected Convolutional Networks
This paper extend ResNet and make network more densely connected.
It proposes that each layer has inputs of all the outputs of its previous layers,
all the outputs are concatenated instead of adding together. 